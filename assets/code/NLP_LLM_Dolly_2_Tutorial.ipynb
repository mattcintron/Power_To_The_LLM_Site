{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dolly 2\n",
        "Databricks team created large language model based on EleutherAI's Pythia model and they later fine-tuned on approximately 15,000 record instruction corpus. It comes under Apache 2 license which means the model, the training code, the dataset, and model weights that it was trained with are all available as open source, such that you can make a commercial use of it to create your own customized large language model.\n",
        "\n",
        "It comes with three sizes - 12B, 7B and 3B parameters."
      ],
      "metadata": {
        "id": "ekXhvSrCRuX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    databricks/dolly-v2-12b on pythia-12b\n",
        "    databricks/dolly-v2-7b  on pythia-6.9b\n",
        "    databricks/dolly-v2-3b  on pythia-2.8b"
      ],
      "metadata": {
        "id": "x88Hj3zISUez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory Requirements : Dolly 2\n",
        "It requires a GPU with roughly 10GB RAM for 7B model with 8-bit quantization. For 12B model, it requires atleast 18GB GPU vRAM."
      ],
      "metadata": {
        "id": "WrQZR2UTSeiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "iOop4fIcZ_jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Load the model"
      ],
      "metadata": {
        "id": "Ri4yA03JMYeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "baseModel = \"databricks/dolly-v2-3b\"\n",
        "#baseModel = \"databricks/dolly-v2-7b\"\n",
        "# baseModel = \"databricks/dolly-v2-12b\"\n",
        "\n",
        "load_8bit = True\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(baseModel, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "generator = pipeline(task='text-generation', model=model, tokenizer=tokenizer,  max_length=200)"
      ],
      "metadata": {
        "id": "SYUGdUv2Rvz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator(\"write me a story\"))"
      ],
      "metadata": {
        "id": "7L5ve5xVjYEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expand responce length max\n",
        "looks like its default length is not so great lets fix that"
      ],
      "metadata": {
        "id": "imobB-VaMODt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(task='text-generation', model=model, tokenizer=tokenizer,  max_length=200)\n",
        "print(generator(\"generate Python code to remove duplicates from dataframe:\"))"
      ],
      "metadata": {
        "id": "W7XJ0P2baSyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}