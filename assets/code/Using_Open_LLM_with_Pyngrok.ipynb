{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## OpenLLM\n",
        "\n",
        "\n",
        "With OpenLLM, you can run inference with any open-source large-language models(LLMs), deploy to the cloud or on-premises, and build powerful AI apps.\n",
        "\n",
        "ðŸš‚ SOTA LLMs: built-in supports a wide range of open-source LLMs and model runtime, including StableLM, Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.\n",
        "\n",
        "ðŸ”¥ Flexible APIs: serve LLMs over RESTful API or gRPC with one command, query via WebUI, CLI, our Python/Javascript client, or any HTTP client.\n",
        "\n",
        "â›“ï¸ Freedom To Build: First-class support for LangChain and BentoML allows you to easily create your own AI apps by composing LLMs with other models and services.\n",
        "\n",
        "ðŸŽ¯ Streamline Deployment: Automatically generate your LLM server Docker Images or deploy as serverless endpoint via â˜ï¸ BentoCloud.\n",
        "\n",
        "ðŸ¤–ï¸ Bring your own LLM: Fine-tune any LLM to suit your needs with LLM.tuning(). (Coming soon)\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/bentoml/OpenLLM"
      ],
      "metadata": {
        "id": "6fKc5AUaGs52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install openllm\n",
        "!pip install mlflow\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "phoZzMgHLDkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "o6kSsKL46x_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = \"1xiKn1eTJOmwpwdB4DtuzRRMXZf_6KBaaCrekZX8Vn7HQjQRP\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O44xia6W74gA",
        "outputId": "d7f62003-ea78-49b6-c263-d39f835f3caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_tunnel = ngrok.connect(addr=\"3000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2k9Vzth8MBd",
        "outputId": "8ebcc182-f935-4df3-cf99-a8ff09060180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-15T21:23:26+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://e749-35-187-254-110.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! openllm -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IkTSBY9L8Fm",
        "outputId": "cec93708-fbbf-4cbb-fae6-224fbeee0136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: openllm [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—\n",
            "  â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘\n",
            "  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘\n",
            "  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘\n",
            "  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘\n",
            "   â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•\n",
            "\n",
            "  An open platform for operating large language models in production.\n",
            "  Fine-tune, serve, deploy, and monitor any LLMs with ease.\n",
            "\n",
            "Options:\n",
            "  -v, --version  Show the version and exit.\n",
            "  -h, --help     Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  build            Package a given models...\n",
            "  download-models  Setup LLM interactively.\n",
            "  models           List all supported models.\n",
            "  prune            Remove all saved models...\n",
            "  query            Ask a LLM interactively,...\n",
            "  start            Start any LLM as a REST...\n",
            "  start-grpc       Start any LLM as a gRPC...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting an LLM Server\n",
        "To start an LLM server, use openllm start. For example, to start a dolly-v2 server:\n",
        "\n",
        "    openllm start dolly-v2\n",
        "Following this, a Web UI will be accessible at http://localhost:3000 where you can experiment with the endpoints and sample input prompts.\n",
        "\n",
        "OpenLLM provides a built-in Python client, allowing you to interact with the model. In a different terminal window or a Jupyter notebook, create a client to start interacting with the model:\n",
        "\n",
        "    >>> import openllm\n",
        "    >>> client = openllm.client.HTTPClient('http://localhost:3000')\n",
        "    >>> client.query('Explain to me the difference between \"further\" and \"farther\"')\n",
        "You can also use the openllm query command to query the model from the terminal:\n",
        "\n",
        "    export OPENLLM_ENDPOINT=http://localhost:3000\n",
        "    openllm query 'Explain to me the difference between \"further\" and \"farther\"'\n",
        "    Visit http://localhost:3000/docs.json for OpenLLM's API specification.\n",
        "\n",
        "Users can also specify different variants of the model to be served, by providing the --model-id argument, e.g.:\n",
        "\n",
        "openllm start flan-t5 --model-id google/flan-t5-large\n",
        "Use the openllm models command to see the list of models and their variants supported in OpenLLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "XY1bBJ4ke_wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm start dolly-v2"
      ],
      "metadata": {
        "id": "wmf_KbltMM4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc62836-ab7e-490e-d7a9-2870b424e9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-15 21:24:14.104037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-3b:\n",
            "- instruct_pipeline.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "2023-06-15T21:27:29+0000 [INFO] [cli] Environ for worker 0: set CUDA_VISIBLE_DEVICES to 0\n",
            "2023-06-15T21:27:29+0000 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"_service.py:svc\" can be accessed at http://localhost:3000/metrics.\n",
            "2023-06-15T21:27:30+0000 [INFO] [cli] Starting production HTTP BentoServer from \"_service.py:svc\" listening on http://0.0.0.0:3000 (Press CTRL+C to quit)\n",
            "2023-06-15 21:27:40.376774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-15T21:28:07+0000 [WARNING] [runner:llm-dolly-v2-runner:1] The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "2023-06-15T21:28:46+0000 [INFO] [api_server:llm-dolly-v2-service:2] 136.226.49.3:0 (scheme=https,method=GET,path=/,type=,length=) (status=200,type=text/html; charset=utf-8,length=2859) 0.506ms (trace=9430b4af100ceea75bb222a865f604a0,span=8ce872da8273e89b,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:46+0000 [INFO] [api_server:llm-dolly-v2-service:2] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/swagger-ui.css,type=,length=) (status=200,type=text/css; charset=utf-8,length=143980) 12.157ms (trace=4528a4ba3ca59d5508ff01a7dfa3dd73,span=f367c7e55704ace6,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:47+0000 [INFO] [api_server:llm-dolly-v2-service:1] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/index.css,type=,length=) (status=200,type=text/css; charset=utf-8,length=1125) 4.168ms (trace=3b320c742f9f2ce48ef973bbf80e3061,span=55688a40dff504c7,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:47+0000 [INFO] [api_server:llm-dolly-v2-service:1] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/swagger-initializer.js,type=,length=) (status=200,type=application/javascript,length=383) 6.906ms (trace=ddd88ccee7966982f7ab0f94604c85b9,span=55a02ea1d273df92,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:47+0000 [INFO] [api_server:llm-dolly-v2-service:2] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/swagger-ui-bundle.js,type=,length=) (status=200,type=application/javascript,length=1096221) 17.359ms (trace=93a116122a125ab8ccccf8996c84baad,span=ca8c176ce314b30d,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:47+0000 [INFO] [api_server:llm-dolly-v2-service:1] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/swagger-ui-standalone-preset.js,type=,length=) (status=200,type=application/javascript,length=339540) 15.904ms (trace=517dba70cbbfef2b1dabe6a75f29d902,span=090351ccb5896ec6,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:49+0000 [INFO] [api_server:llm-dolly-v2-service:1] 136.226.49.3:0 (scheme=https,method=GET,path=/static_content/favicon-32x32.png,type=,length=) (status=200,type=image/png,length=1912) 6.197ms (trace=974977e4be1a4aad2bd2d73c9e975890,span=04aaff9c4f46bd7c,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:28:49+0000 [INFO] [api_server:llm-dolly-v2-service:2] 136.226.49.3:0 (scheme=https,method=GET,path=/docs.json,type=,length=) (status=200,type=application/json,length=6961) 25.680ms (trace=afdbe04b913225de001b2633dc30995a,span=6fe63ee90e2614b5,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:29:39+0000 [INFO] [runner:llm-dolly-v2-runner:1] _ (scheme=http,method=POST,path=/generate,type=application/octet-stream,length=880) (status=200,type=application/vnd.bentoml.DefaultContainer,length=63) 2863.551ms (trace=aaf84bb257857aaf9feffebc06d3689f,span=be97c6a8a9460351,sampled=1,service.name=llm-dolly-v2-runner)\n",
            "2023-06-15T21:29:39+0000 [INFO] [api_server:llm-dolly-v2-service:1] 136.226.49.3:0 (scheme=https,method=POST,path=/v1/generate,type=application/json,length=817) (status=200,type=application/json,length=637) 3012.470ms (trace=aaf84bb257857aaf9feffebc06d3689f,span=c37ee9bfc400ca71,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:30:26+0000 [INFO] [runner:llm-dolly-v2-runner:1] _ (scheme=http,method=POST,path=/generate,type=application/octet-stream,length=893) (status=200,type=application/vnd.bentoml.DefaultContainer,length=1122) 14476.587ms (trace=3de03cc9ba70f126ad1a9a49ac1156e9,span=195c296c307c1458,sampled=1,service.name=llm-dolly-v2-runner)\n",
            "2023-06-15T21:30:26+0000 [INFO] [api_server:llm-dolly-v2-service:2] 136.226.49.3:0 (scheme=https,method=POST,path=/v1/generate,type=application/json,length=831) (status=200,type=application/json,length=1701) 14558.949ms (trace=3de03cc9ba70f126ad1a9a49ac1156e9,span=75f0ea507c62a6c6,sampled=1,service.name=llm-dolly-v2-service)\n",
            "2023-06-15T21:34:08+0000 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd8431ffa60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 919, in _run\n",
            "    val = self.callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/circus/util.py\", line 1038, in wrapper\n",
            "    raise ConflictError(\"arbiter is already running %s command\"\n",
            "circus.exc.ConflictError: arbiter is already running arbiter_stop command\n",
            "\u001b[34m\n",
            "ðŸš€ Next step: run 'openllm build dolly-v2' to create a Bento for dolly-v2\u001b[0m\n",
            "2023-06-15T21:34:09+0000 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd8431ffa60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 919, in _run\n",
            "    val = self.callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/circus/util.py\", line 1038, in wrapper\n",
            "    raise ConflictError(\"arbiter is already running %s command\"\n",
            "circus.exc.ConflictError: arbiter is already running arbiter_stop command\n",
            "2023-06-15T21:34:10+0000 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd8431ffa60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 919, in _run\n",
            "    val = self.callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/circus/util.py\", line 1038, in wrapper\n",
            "    raise ConflictError(\"arbiter is already running %s command\"\n",
            "circus.exc.ConflictError: arbiter is already running arbiter_stop command\n",
            "2023-06-15T21:34:11+0000 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd8431ffa60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 919, in _run\n",
            "    val = self.callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/circus/util.py\", line 1038, in wrapper\n",
            "    raise ConflictError(\"arbiter is already running %s command\"\n",
            "circus.exc.ConflictError: arbiter is already running arbiter_stop command\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openllm\n",
        "client = openllm.client.HTTPClient('http://localhost:3000')\n",
        "client.query(\"Explain to me the difference between further and farther\")"
      ],
      "metadata": {
        "id": "mWZcADNvgfLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export OPENLLM_ENDPOINT=http://localhost:3000\n",
        "!openllm query 'Explain to me the difference between \"further\" and \"farther\"'"
      ],
      "metadata": {
        "id": "gkQn__CqhPMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}